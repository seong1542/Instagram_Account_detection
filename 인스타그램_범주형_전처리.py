# -*- coding: utf-8 -*-
"""인스타그램_범주형_전처리.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zJWFFcXzh7sOI1yKIUoEnI2DjVkhO7SA
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

filepath='/content/gdrive/My Drive/'

train = pd.read_csv(filepath+'instagram_detection.csv')
print(train.shape)

train.head()

train.info()

# 프로필 유무, 이름과 아이디가 같은가, url 유무, 비공개 유무, 프로페셔널 계정 유무, 가짜계정 유무는 범주형으로 설정
train['#profile_pic'] = train['#profile_pic'].astype('category')
train['#name==id'] = train['#name==id'].astype('category')
train['#url'] = train['#url'].astype('category')
train['#private'] = train['#private'].astype('category')
train['#professional'] = train['#professional'].astype('category')
train['#fake'] = train['#fake'].astype('category')
train['#highlight'] = train['#highlight'].astype('category')

train.head()

train.info()

train.nunique() # 혹시 0과 1 만 들어있어야 할 곳에 다른 것이 들어 갔는지 확인

train.head(10)

# train 셋에 있는 fake는 y에 저장
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)

y_train.head()

X_train.head()

data_corr = X_train.corr(method='pearson')
axes = sns.heatmap(data_corr, vmin=-1, vmax=1, cmap='BrBG')
axes.set_title('Correlation Heatmap Between Features')

# 진짜 계정과 가짜 계정이 총 몇 개씩 있는지 확인 (False = 진짜 계정 , True = 가짜 계정)
unique, freq = np.unique(y_train, return_counts=True)
for i, j in zip(unique, freq):
  print('Label : ',i, ', Frequency : ', j)

# train과 validation set은 8:2로 나눔
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)

from sklearn.ensemble import RandomForestClassifier
rf= RandomForestClassifier()
rf.fit(X_train, y_train)
train_score = rf.score(X_train, y_train)
valid_score = rf.score(X_valid, y_valid)
print('RandomForest Train_Accuracy : {}'.format(train_score))
print('RandomForest Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.ensemble import GradientBoostingClassifier
gb= GradientBoostingClassifier()
gb.fit(X_train, y_train)
train_score = gb.score(X_train, y_train)
valid_score = gb.score(X_valid, y_valid)
print('GradientBoosting Train_Accuracy : {}'.format(train_score))
print('GradientBoosting Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.linear_model import LogisticRegression
lg = LogisticRegression(penalty = 'l2',solver='newton-cg',random_state=0, max_iter=600)
lg.fit(X_train,y_train)
train_score = lg.score(X_train,y_train)
valid_score = lg.score(X_valid,y_valid)
print('Logistic Train_Accuracy : {}'.format(train_score))
print('Logistic Valid_Accuracy : {}'.format(valid_score))
print()
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train,y_train)
train_score = dt.score(X_train,y_train)
valid_score=dt.score(X_valid,y_valid)
print('DecisionTree Train_Accuracy : {}'.format(train_score))
print('DecisionTree Valid_Accuracy : {}'.format(valid_score))

#XGBoost 시도해보니, category는 안받아줌 -> bool으로 시도해봄.
train['#profile_pic'] = train['#profile_pic'].astype(bool)
train['#name==id'] = train['#name==id'].astype(bool)
train['#url'] = train['#url'].astype(bool)
train['#private'] = train['#private'].astype(bool)
train['#professional'] = train['#professional'].astype(bool)
train['#fake'] = train['#fake'].astype(bool)
train['#highlight'] = train['#highlight'].astype(bool)

data_corr = X_train.corr(method='pearson')
axes = sns.heatmap(data_corr, vmin=-1, vmax=1, cmap='BrBG')
axes.set_title('Correlation Heatmap Between Features')

train.info()

y_train = train['#fake']
X_train = train.drop('#fake',axis=1)

X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)

import xgboost as xgb
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train,y_train)
train_score = xgb_model.score(X_train,y_train)
valid_score = xgb_model.score(X_valid,y_valid)
print('XGBoost Train_Accuracy : {}'.format(train_score))
print('XGBoost Valid_Accuracy : {}'.format(valid_score))
print()

import lightgbm as lgb
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train, y_train)
train_score =lgb_model.score(X_train,y_train)
valid_score= lgb_model.score(X_valid,y_valid)
print('LGBM Train_Accuracy : {}'.format(train_score))
print('LGBM Valid_Accuracy : {}'.format(valid_score))
print()
from sklearn.ensemble import RandomForestClassifier
rf= RandomForestClassifier()
rf.fit(X_train, y_train)
train_score = rf.score(X_train, y_train)
valid_score = rf.score(X_valid, y_valid)
print('RandomForest Train_Accuracy : {}'.format(train_score))
print('RandomForest Valid_Accuracy : {}'.format(valid_score))

"""# int형으로만 해놓고 돌리면?"""

train= pd.read_csv(filepath+'instagram_detection.csv')  # int형으로 그냥 두고 돌릴 때 변화가 있는지 확인
print(train.shape)

train.info()

data_corr = train.corr(method='pearson')
axes = sns.heatmap(data_corr, vmin=-1, vmax=1, cmap='BrBG')
axes.set_title('Correlation Heatmap Between Features')

y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)

from sklearn.ensemble import RandomForestClassifier
rf= RandomForestClassifier()
rf.fit(X_train, y_train)
train_score = rf.score(X_train, y_train)
valid_score = rf.score(X_valid, y_valid)
print('RandomForest Train_Accuracy : {}'.format(train_score))
print('RandomForest Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.ensemble import GradientBoostingClassifier
gb= GradientBoostingClassifier()
gb.fit(X_train, y_train)
train_score = gb.score(X_train, y_train)
valid_score = gb.score(X_valid, y_valid)
print('GradientBoosting Train_Accuracy : {}'.format(train_score))
print('GradientBoosting Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.linear_model import LogisticRegression
lg = LogisticRegression(penalty = 'l2',solver='newton-cg',random_state=0, max_iter=600)
lg.fit(X_train,y_train)
train_score = lg.score(X_train,y_train)
valid_score = lg.score(X_valid,y_valid)
print('Logistic Train_Accuracy : {}'.format(train_score))
print('Logistic Valid_Accuracy : {}'.format(valid_score))
print()

import xgboost as xgb
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train,y_train)
train_score = xgb_model.score(X_train,y_train)
valid_score = xgb_model.score(X_valid,y_valid)
print('XGBoost Train_Accuracy : {}'.format(train_score))
print('XGBoost Valid_Accuracy : {}'.format(valid_score))
print()

import lightgbm as lgb
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train, y_train)
train_score =lgb_model.score(X_train,y_train)
valid_score= lgb_model.score(X_valid,y_valid)
print('LGBM Train_Accuracy : {}'.format(train_score))
print('LGBM Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train,y_train)
train_score = dt.score(X_train,y_train)
valid_score=dt.score(X_valid,y_valid)
print('DecisionTree Train_Accuracy : {}'.format(train_score))
print('DecisionTree Valid_Accuracy : {}'.format(valid_score))

"""# 최대한 효율적인 방법 - RandomForest, GradientBoosting, LGBM
## (Train 정확도 높은 것, valid 정확도 높은 것 중 고려)

## **int형으로 돌릴 때**, *Randomforest*에서 정확도 올라감 (나머지는 변함X) 
## -> **int형으로 유지한 상태에서, optuna 사용해봄.**
"""

!pip install optuna #옵튜나 설치

import optuna
from sklearn.metrics import log_loss
categorical_features = ['#profile_pic','#name==id','#url','#private','#highlight','#professional']
def objective(trial):
  params = {'objective':'binary', 
            'n_estimators' : trial.suggest_categorical('n_estimators', [300,500,800,1000]),
            'learning_rate' : trial.suggest_categorical('lerning_rate', [0.05,0.01,0.1]), 
            'max_depth' : trial.suggest_categorical('max_depth', [5,6,7,8]), 
            'max_features' : trial.suggest_categorical('max_features', [0.1,0.3,0.5,0.7]), 
            'min_samples_split' : trial.suggest_categorical('min_samples_split', [2,6]),
            'min_samples_leaf' : trial.suggest_categorical('min_samples_leaf', [2,6])
            }
  lgb_train = lgb.Dataset(X_train,y_train, categorical_feature = categorical_features)
  lgb_eval = lgb.Dataset(X_valid,y_valid,reference=lgb_train, categorical_feature = categorical_features)
  model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_eval],verbose_eval=10, num_boost_round=1000, early_stopping_rounds=100)
  y_pred_valid = model.predict(X_valid,num_iteration=model.best_iteration)
  score = log_loss(y_valid,y_pred_valid)
  return score

study=optuna.create_study(direction='minimize')
study.optimize(objective,n_trials=40) #5회반복

print('Best score : '+str(study.best_trial.value))

study.best_params

import lightgbm as lgb
lgb_model = lgb.LGBMClassifier(**study.best_params)
lgb_model.fit(X_train, y_train)
train_score =lgb_model.score(X_train,y_train)
valid_score= lgb_model.score(X_valid,y_valid)
print('LGBM Train_Accuracy : {}'.format(train_score))
print('LGBM Valid_Accuracy : {}'.format(valid_score))

"""# 결과인 fake부분만 Bool타입으로 지정"""

train = pd.read_csv(filepath+'instagram_detection.csv')
print(train.shape)

train['#fake'] = train['#fake'].astype('bool')

from sklearn.model_selection import train_test_split
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)

from sklearn.ensemble import RandomForestClassifier
rf= RandomForestClassifier()
rf.fit(X_train, y_train)
train_score = rf.score(X_train, y_train)
valid_score = rf.score(X_valid, y_valid)
print('RandomForest Train_Accuracy : {}'.format(train_score))
print('RandomForest Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.ensemble import GradientBoostingClassifier
gb= GradientBoostingClassifier()
gb.fit(X_train, y_train)
train_score = gb.score(X_train, y_train)
valid_score = gb.score(X_valid, y_valid)
print('GradientBoosting Train_Accuracy : {}'.format(train_score))
print('GradientBoosting Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.linear_model import LogisticRegression
lg = LogisticRegression(penalty = 'l2',solver='newton-cg',random_state=0, max_iter=600)
lg.fit(X_train,y_train)
train_score = lg.score(X_train,y_train)
valid_score = lg.score(X_valid,y_valid)
print('Logistic Train_Accuracy : {}'.format(train_score))
print('Logistic Valid_Accuracy : {}'.format(valid_score))
print()

import xgboost as xgb
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train,y_train)
train_score = xgb_model.score(X_train,y_train)
valid_score = xgb_model.score(X_valid,y_valid)
print('XGBoost Train_Accuracy : {}'.format(train_score))
print('XGBoost Valid_Accuracy : {}'.format(valid_score))
print()

import lightgbm as lgb
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train, y_train)
train_score =lgb_model.score(X_train,y_train)
valid_score= lgb_model.score(X_valid,y_valid)
print('LGBM Train_Accuracy : {}'.format(train_score))
print('LGBM Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train,y_train)
train_score = dt.score(X_train,y_train)
valid_score=dt.score(X_valid,y_valid)
print('DecisionTree Train_Accuracy : {}'.format(train_score))
print('DecisionTree Valid_Accuracy : {}'.format(valid_score))

"""로지스틱회귀분석 valid 정확도에서 살짝 오른 것을 제외하면, 나머지 정확도는 모든게 int형일 때, 더 정확하거나 비슷하다. ->굳이 Bool을 써야할 필요가 없다.

# 결과
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
print(train.shape)

y_train = train['#fake']
X_train = train.drop('#fake',axis=1)

from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)

from sklearn.ensemble import RandomForestClassifier
rf= RandomForestClassifier(random_state=11)
rf.fit(X_train, y_train)
train_score = rf.score(X_train, y_train)
valid_score = rf.score(X_valid, y_valid)
print('RandomForest Train_Accuracy : {}'.format(train_score))
print('RandomForest Valid_Accuracy : {}'.format(valid_score))
print()

import lightgbm as lgb
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train, y_train)
train_score =lgb_model.score(X_train,y_train)
valid_score= lgb_model.score(X_valid,y_valid)
print('LGBM Train_Accuracy : {}'.format(train_score))
print('LGBM Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.ensemble import GradientBoostingClassifier
gb= GradientBoostingClassifier()
gb.fit(X_train, y_train)
train_score = gb.score(X_train, y_train)
valid_score = gb.score(X_valid, y_valid)
print('GradientBoosting Train_Accuracy : {}'.format(train_score))
print('GradientBoosting Valid_Accuracy : {}'.format(valid_score))
print()

#가짜계정인가? -> False = 진짜계정, True = 가짜계정
p=[[1,19,4,14,0,0,0,0,0,0,3,3,0,0,0,0]]  # 학습하지 않은, 가짜 계정의 정보를 입력
rf_y = rf.predict(p)
lgb_y=lgb_model.predict(p)
gb_y=gb.predict(p)
print(rf_y)
print(lgb_y)
print(gb_y)    #가짜계정이라고 답변 내리는 것을 볼 수 있음.

#가짜계정인가? -> False= 진짜계정, True = 가짜계정
x=[[0,9,0,3,0,0,0,0,1,4,265,350,0,0,0,0]]  # 학습하지 않은, 진짜 계정의 정보를 입력
rf_y = rf.predict(x)
lgb_y=lgb_model.predict(x)
gb_y=gb.predict(x)
print(rf_y)
print(lgb_y)
print(gb_y)    #진짜계정이라고 답변 내리는 것을 볼 수 있음.

