{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Instagram_DeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlVRHPq6pA-H",
        "outputId": "ce14cf5f-62eb-4d6b-d5d4-758d7442b433"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBeLf0JfsSDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "951b809e-8586-4706-efd6-967fd5fc46d6"
      },
      "source": [
        "filepath='/content/gdrive/My Drive/'\n",
        "train = pd.read_csv(filepath+'instagram_train.csv')\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train)\n",
        "train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)\n",
        "y_train = train['#fake']\n",
        "X_train = train.drop('#fake',axis=1)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)\n",
        "\n",
        "print('X_train = ', X_train.shape)\n",
        "print('X_valid = ', X_valid.shape)\n",
        "print('y_train = ', y_train.shape)\n",
        "print('y_valid = ', y_valid.shape)\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
        "y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)\n",
        "print(y_train.shape, y_valid.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train =  (8000, 16)\n",
            "X_valid =  (2000, 16)\n",
            "y_train =  (8000,)\n",
            "y_valid =  (2000,)\n",
            "(8000, 2) (2000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRFd_RPO2Ddc"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
        "\n",
        "def dnn_model():\n",
        "  model=Sequential()\n",
        "\n",
        "  #hidden layer #1\n",
        "  model.add(Dense(units = (256), input_dim = 16))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.2))  \n",
        "  #hidden layer #2\n",
        "  model.add(Dense(units = (128)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.2))  \n",
        "  #hidden layer #3\n",
        "  model.add(Dense(units = (64)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.2))  \n",
        "  #hidden layer #4\n",
        "  model.add(Dense(units = (32)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.2))  \n",
        "  #hidden layer #5\n",
        "  model.add(Dense(units = (16)))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  #output layer\n",
        "  model.add(Dense(units=2))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtwySWuW2F4D",
        "outputId": "208a6c45-bb83-41de-9937-e5cf6673c64a"
      },
      "source": [
        "model =dnn_model()\n",
        "model.summary()\n",
        "opti = tf.keras.optimizers.Adam(lr=0.001)\n",
        "checkpoint_path = \"model_checkpoint.ckpt\"  #best model save\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, save_weight_only=True,save_best_only=True, monitor=\"val_loss\",verbose=2)\n",
        "model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 256)               4352      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 34        \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 50,066\n",
            "Trainable params: 49,106\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YhbOEkK2Ico",
        "outputId": "9ed06b92-cf2e-4f54-e4eb-0a78b4203394"
      },
      "source": [
        "hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False,callbacks=[checkpoint],)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "125/125 [==============================] - 2s 6ms/step - loss: 0.4817 - accuracy: 0.7874 - val_loss: 0.5615 - val_accuracy: 0.6615\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.56145, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 2/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.3341 - accuracy: 0.8624 - val_loss: 0.3742 - val_accuracy: 0.8450\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.56145 to 0.37416, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 3/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.3116 - accuracy: 0.8669 - val_loss: 0.3008 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.37416 to 0.30082, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 4/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.3018 - accuracy: 0.8736 - val_loss: 0.2637 - val_accuracy: 0.8895\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.30082 to 0.26375, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 5/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2925 - accuracy: 0.8781 - val_loss: 0.2552 - val_accuracy: 0.8910\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.26375 to 0.25520, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 6/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2879 - accuracy: 0.8767 - val_loss: 0.2568 - val_accuracy: 0.8840\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.25520\n",
            "Epoch 7/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2751 - accuracy: 0.8854 - val_loss: 0.2357 - val_accuracy: 0.8945\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.25520 to 0.23569, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 8/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2690 - accuracy: 0.8885 - val_loss: 0.2223 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.23569 to 0.22227, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 9/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2602 - accuracy: 0.8932 - val_loss: 0.2282 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.22227\n",
            "Epoch 10/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2590 - accuracy: 0.8934 - val_loss: 0.2165 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.22227 to 0.21651, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 11/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2475 - accuracy: 0.8957 - val_loss: 0.2120 - val_accuracy: 0.9155\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.21651 to 0.21201, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 12/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9019 - val_loss: 0.2130 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.21201\n",
            "Epoch 13/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2412 - accuracy: 0.8996 - val_loss: 0.2140 - val_accuracy: 0.9135\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.21201\n",
            "Epoch 14/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2376 - accuracy: 0.9039 - val_loss: 0.2148 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.21201\n",
            "Epoch 15/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2370 - accuracy: 0.9018 - val_loss: 0.2126 - val_accuracy: 0.9115\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.21201\n",
            "Epoch 16/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2338 - accuracy: 0.9013 - val_loss: 0.2068 - val_accuracy: 0.9175\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.21201 to 0.20679, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 17/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2307 - accuracy: 0.9039 - val_loss: 0.2091 - val_accuracy: 0.9140\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.20679\n",
            "Epoch 18/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2283 - accuracy: 0.9071 - val_loss: 0.2094 - val_accuracy: 0.9175\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.20679\n",
            "Epoch 19/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2277 - accuracy: 0.9068 - val_loss: 0.2057 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.20679 to 0.20567, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 20/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2273 - accuracy: 0.9084 - val_loss: 0.2076 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.20567\n",
            "Epoch 21/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2205 - accuracy: 0.9099 - val_loss: 0.2131 - val_accuracy: 0.9155\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.20567\n",
            "Epoch 22/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2208 - accuracy: 0.9133 - val_loss: 0.2095 - val_accuracy: 0.9125\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.20567\n",
            "Epoch 23/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2194 - accuracy: 0.9078 - val_loss: 0.2119 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.20567\n",
            "Epoch 24/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2182 - accuracy: 0.9134 - val_loss: 0.2123 - val_accuracy: 0.9130\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.20567\n",
            "Epoch 25/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2134 - accuracy: 0.9159 - val_loss: 0.2115 - val_accuracy: 0.9135\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.20567\n",
            "Epoch 26/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2138 - accuracy: 0.9135 - val_loss: 0.2041 - val_accuracy: 0.9175\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.20567 to 0.20407, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 27/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2100 - accuracy: 0.9154 - val_loss: 0.2138 - val_accuracy: 0.9170\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.20407\n",
            "Epoch 28/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2085 - accuracy: 0.9162 - val_loss: 0.2054 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.20407\n",
            "Epoch 29/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.2075 - accuracy: 0.9166 - val_loss: 0.2090 - val_accuracy: 0.9155\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.20407\n",
            "Epoch 30/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2038 - accuracy: 0.9150 - val_loss: 0.2200 - val_accuracy: 0.9115\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.20407\n",
            "Epoch 31/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2070 - accuracy: 0.9171 - val_loss: 0.2059 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.20407\n",
            "Epoch 32/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2045 - accuracy: 0.9179 - val_loss: 0.2141 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.20407\n",
            "Epoch 33/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2045 - accuracy: 0.9165 - val_loss: 0.2129 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.20407\n",
            "Epoch 34/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1967 - accuracy: 0.9211 - val_loss: 0.2080 - val_accuracy: 0.9195\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.20407\n",
            "Epoch 35/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.2015 - accuracy: 0.9162 - val_loss: 0.2133 - val_accuracy: 0.9175\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.20407\n",
            "Epoch 36/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1983 - accuracy: 0.9164 - val_loss: 0.2042 - val_accuracy: 0.9190\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.20407\n",
            "Epoch 37/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1976 - accuracy: 0.9208 - val_loss: 0.2047 - val_accuracy: 0.9250\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.20407\n",
            "Epoch 38/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1927 - accuracy: 0.9193 - val_loss: 0.2118 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.20407\n",
            "Epoch 39/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1874 - accuracy: 0.9205 - val_loss: 0.2069 - val_accuracy: 0.9215\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.20407\n",
            "Epoch 40/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1929 - accuracy: 0.9216 - val_loss: 0.2100 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.20407\n",
            "Epoch 41/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1906 - accuracy: 0.9234 - val_loss: 0.2096 - val_accuracy: 0.9225\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.20407\n",
            "Epoch 42/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1918 - accuracy: 0.9234 - val_loss: 0.1997 - val_accuracy: 0.9225\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.20407 to 0.19968, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 43/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1885 - accuracy: 0.9231 - val_loss: 0.2058 - val_accuracy: 0.9215\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.19968\n",
            "Epoch 44/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1858 - accuracy: 0.9202 - val_loss: 0.2073 - val_accuracy: 0.9235\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.19968\n",
            "Epoch 45/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1880 - accuracy: 0.9246 - val_loss: 0.2057 - val_accuracy: 0.9210\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.19968\n",
            "Epoch 46/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1832 - accuracy: 0.9261 - val_loss: 0.2159 - val_accuracy: 0.9205\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.19968\n",
            "Epoch 47/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1813 - accuracy: 0.9268 - val_loss: 0.2016 - val_accuracy: 0.9265\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.19968\n",
            "Epoch 48/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1785 - accuracy: 0.9260 - val_loss: 0.2056 - val_accuracy: 0.9240\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.19968\n",
            "Epoch 49/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1799 - accuracy: 0.9294 - val_loss: 0.2200 - val_accuracy: 0.9145\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.19968\n",
            "Epoch 50/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1793 - accuracy: 0.9287 - val_loss: 0.2061 - val_accuracy: 0.9235\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.19968\n",
            "Epoch 51/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1748 - accuracy: 0.9295 - val_loss: 0.2036 - val_accuracy: 0.9225\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.19968\n",
            "Epoch 52/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1775 - accuracy: 0.9275 - val_loss: 0.2071 - val_accuracy: 0.9225\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.19968\n",
            "Epoch 53/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1752 - accuracy: 0.9276 - val_loss: 0.2041 - val_accuracy: 0.9275\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.19968\n",
            "Epoch 54/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1737 - accuracy: 0.9310 - val_loss: 0.2008 - val_accuracy: 0.9265\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.19968\n",
            "Epoch 55/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1759 - accuracy: 0.9295 - val_loss: 0.2053 - val_accuracy: 0.9280\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.19968\n",
            "Epoch 56/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1681 - accuracy: 0.9317 - val_loss: 0.2079 - val_accuracy: 0.9215\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.19968\n",
            "Epoch 57/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1692 - accuracy: 0.9317 - val_loss: 0.1977 - val_accuracy: 0.9305\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.19968 to 0.19775, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 58/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1729 - accuracy: 0.9308 - val_loss: 0.2174 - val_accuracy: 0.9270\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.19775\n",
            "Epoch 59/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1705 - accuracy: 0.9320 - val_loss: 0.2072 - val_accuracy: 0.9260\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.19775\n",
            "Epoch 60/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1666 - accuracy: 0.9334 - val_loss: 0.2095 - val_accuracy: 0.9280\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.19775\n",
            "Epoch 61/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1683 - accuracy: 0.9350 - val_loss: 0.2101 - val_accuracy: 0.9260\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.19775\n",
            "Epoch 62/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1629 - accuracy: 0.9330 - val_loss: 0.2037 - val_accuracy: 0.9275\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.19775\n",
            "Epoch 63/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1645 - accuracy: 0.9337 - val_loss: 0.1950 - val_accuracy: 0.9295\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.19775 to 0.19495, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 64/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1639 - accuracy: 0.9341 - val_loss: 0.1972 - val_accuracy: 0.9305\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.19495\n",
            "Epoch 65/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1597 - accuracy: 0.9358 - val_loss: 0.2050 - val_accuracy: 0.9295\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.19495\n",
            "Epoch 66/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1636 - accuracy: 0.9327 - val_loss: 0.2123 - val_accuracy: 0.9225\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.19495\n",
            "Epoch 67/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1635 - accuracy: 0.9341 - val_loss: 0.2141 - val_accuracy: 0.9300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.19495\n",
            "Epoch 68/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1601 - accuracy: 0.9374 - val_loss: 0.2015 - val_accuracy: 0.9275\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.19495\n",
            "Epoch 69/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1582 - accuracy: 0.9389 - val_loss: 0.2082 - val_accuracy: 0.9285\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.19495\n",
            "Epoch 70/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1594 - accuracy: 0.9354 - val_loss: 0.1952 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.19495\n",
            "Epoch 71/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1558 - accuracy: 0.9359 - val_loss: 0.2020 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.19495\n",
            "Epoch 72/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1578 - accuracy: 0.9342 - val_loss: 0.2056 - val_accuracy: 0.9295\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.19495\n",
            "Epoch 73/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1526 - accuracy: 0.9379 - val_loss: 0.2064 - val_accuracy: 0.9250\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.19495\n",
            "Epoch 74/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1564 - accuracy: 0.9365 - val_loss: 0.1948 - val_accuracy: 0.9320\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.19495 to 0.19479, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 75/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1541 - accuracy: 0.9386 - val_loss: 0.2019 - val_accuracy: 0.9270\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.19479\n",
            "Epoch 76/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1534 - accuracy: 0.9389 - val_loss: 0.2029 - val_accuracy: 0.9290\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.19479\n",
            "Epoch 77/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1506 - accuracy: 0.9365 - val_loss: 0.2090 - val_accuracy: 0.9305\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.19479\n",
            "Epoch 78/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1520 - accuracy: 0.9377 - val_loss: 0.2047 - val_accuracy: 0.9275\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.19479\n",
            "Epoch 79/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1510 - accuracy: 0.9401 - val_loss: 0.2006 - val_accuracy: 0.9300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.19479\n",
            "Epoch 80/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1479 - accuracy: 0.9401 - val_loss: 0.1984 - val_accuracy: 0.9285\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.19479\n",
            "Epoch 81/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1490 - accuracy: 0.9384 - val_loss: 0.1863 - val_accuracy: 0.9360\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.19479 to 0.18634, saving model to model_checkpoint.ckpt\n",
            "INFO:tensorflow:Assets written to: model_checkpoint.ckpt/assets\n",
            "Epoch 82/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1492 - accuracy: 0.9415 - val_loss: 0.1963 - val_accuracy: 0.9315\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.18634\n",
            "Epoch 83/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1501 - accuracy: 0.9388 - val_loss: 0.2048 - val_accuracy: 0.9295\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.18634\n",
            "Epoch 84/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1453 - accuracy: 0.9433 - val_loss: 0.2030 - val_accuracy: 0.9260\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.18634\n",
            "Epoch 85/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1420 - accuracy: 0.9430 - val_loss: 0.1993 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.18634\n",
            "Epoch 86/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1416 - accuracy: 0.9424 - val_loss: 0.2056 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.18634\n",
            "Epoch 87/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1489 - accuracy: 0.9410 - val_loss: 0.2088 - val_accuracy: 0.9240\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.18634\n",
            "Epoch 88/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1472 - accuracy: 0.9409 - val_loss: 0.2004 - val_accuracy: 0.9290\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.18634\n",
            "Epoch 89/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1436 - accuracy: 0.9405 - val_loss: 0.2077 - val_accuracy: 0.9305\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.18634\n",
            "Epoch 90/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1382 - accuracy: 0.9427 - val_loss: 0.1967 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.18634\n",
            "Epoch 91/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1385 - accuracy: 0.9463 - val_loss: 0.1951 - val_accuracy: 0.9360\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.18634\n",
            "Epoch 92/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1395 - accuracy: 0.9441 - val_loss: 0.2012 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.18634\n",
            "Epoch 93/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1371 - accuracy: 0.9438 - val_loss: 0.2031 - val_accuracy: 0.9320\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.18634\n",
            "Epoch 94/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1396 - accuracy: 0.9446 - val_loss: 0.2019 - val_accuracy: 0.9320\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.18634\n",
            "Epoch 95/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1423 - accuracy: 0.9449 - val_loss: 0.2043 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.18634\n",
            "Epoch 96/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1394 - accuracy: 0.9456 - val_loss: 0.1949 - val_accuracy: 0.9305\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.18634\n",
            "Epoch 97/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1346 - accuracy: 0.9452 - val_loss: 0.2128 - val_accuracy: 0.9320\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.18634\n",
            "Epoch 98/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1367 - accuracy: 0.9440 - val_loss: 0.2177 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.18634\n",
            "Epoch 99/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1355 - accuracy: 0.9459 - val_loss: 0.2025 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.18634\n",
            "Epoch 100/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1378 - accuracy: 0.9455 - val_loss: 0.1999 - val_accuracy: 0.9360\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.18634\n",
            "Epoch 101/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1304 - accuracy: 0.9484 - val_loss: 0.2073 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.18634\n",
            "Epoch 102/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1293 - accuracy: 0.9473 - val_loss: 0.2082 - val_accuracy: 0.9345\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.18634\n",
            "Epoch 103/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1273 - accuracy: 0.9492 - val_loss: 0.2027 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.18634\n",
            "Epoch 104/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1284 - accuracy: 0.9474 - val_loss: 0.2079 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.18634\n",
            "Epoch 105/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1368 - accuracy: 0.9440 - val_loss: 0.1986 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.18634\n",
            "Epoch 106/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1307 - accuracy: 0.9463 - val_loss: 0.2047 - val_accuracy: 0.9350\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.18634\n",
            "Epoch 107/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1319 - accuracy: 0.9466 - val_loss: 0.2095 - val_accuracy: 0.9370\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.18634\n",
            "Epoch 108/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1260 - accuracy: 0.9495 - val_loss: 0.2175 - val_accuracy: 0.9305\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.18634\n",
            "Epoch 109/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1300 - accuracy: 0.9479 - val_loss: 0.2076 - val_accuracy: 0.9345\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.18634\n",
            "Epoch 110/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1232 - accuracy: 0.9510 - val_loss: 0.2007 - val_accuracy: 0.9395\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.18634\n",
            "Epoch 111/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1307 - accuracy: 0.9480 - val_loss: 0.2098 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.18634\n",
            "Epoch 112/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1278 - accuracy: 0.9484 - val_loss: 0.2097 - val_accuracy: 0.9360\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.18634\n",
            "Epoch 113/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1215 - accuracy: 0.9516 - val_loss: 0.2112 - val_accuracy: 0.9350\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.18634\n",
            "Epoch 114/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1199 - accuracy: 0.9510 - val_loss: 0.2098 - val_accuracy: 0.9365\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.18634\n",
            "Epoch 115/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1205 - accuracy: 0.9525 - val_loss: 0.2057 - val_accuracy: 0.9355\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.18634\n",
            "Epoch 116/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1187 - accuracy: 0.9510 - val_loss: 0.2116 - val_accuracy: 0.9370\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.18634\n",
            "Epoch 117/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1265 - accuracy: 0.9494 - val_loss: 0.2163 - val_accuracy: 0.9315\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.18634\n",
            "Epoch 118/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1197 - accuracy: 0.9503 - val_loss: 0.1926 - val_accuracy: 0.9405\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.18634\n",
            "Epoch 119/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1169 - accuracy: 0.9516 - val_loss: 0.2078 - val_accuracy: 0.9365\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.18634\n",
            "Epoch 120/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1172 - accuracy: 0.9530 - val_loss: 0.2172 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.18634\n",
            "Epoch 121/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1176 - accuracy: 0.9525 - val_loss: 0.2151 - val_accuracy: 0.9360\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.18634\n",
            "Epoch 122/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1140 - accuracy: 0.9535 - val_loss: 0.2090 - val_accuracy: 0.9370\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.18634\n",
            "Epoch 123/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1231 - accuracy: 0.9500 - val_loss: 0.2095 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.18634\n",
            "Epoch 124/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1204 - accuracy: 0.9517 - val_loss: 0.2142 - val_accuracy: 0.9390\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.18634\n",
            "Epoch 125/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1183 - accuracy: 0.9541 - val_loss: 0.2069 - val_accuracy: 0.9355\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.18634\n",
            "Epoch 126/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1200 - accuracy: 0.9526 - val_loss: 0.2222 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.18634\n",
            "Epoch 127/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1132 - accuracy: 0.9540 - val_loss: 0.2204 - val_accuracy: 0.9355\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.18634\n",
            "Epoch 128/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1202 - accuracy: 0.9517 - val_loss: 0.2111 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.18634\n",
            "Epoch 129/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1156 - accuracy: 0.9530 - val_loss: 0.2162 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.18634\n",
            "Epoch 130/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1134 - accuracy: 0.9545 - val_loss: 0.2116 - val_accuracy: 0.9335\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.18634\n",
            "Epoch 131/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1133 - accuracy: 0.9546 - val_loss: 0.2119 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.18634\n",
            "Epoch 132/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1098 - accuracy: 0.9545 - val_loss: 0.2166 - val_accuracy: 0.9350\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.18634\n",
            "Epoch 133/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1155 - accuracy: 0.9523 - val_loss: 0.2129 - val_accuracy: 0.9365\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.18634\n",
            "Epoch 134/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.1114 - accuracy: 0.9540 - val_loss: 0.2338 - val_accuracy: 0.9320\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.18634\n",
            "Epoch 135/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1137 - accuracy: 0.9560 - val_loss: 0.2148 - val_accuracy: 0.9355\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.18634\n",
            "Epoch 136/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1196 - accuracy: 0.9514 - val_loss: 0.2053 - val_accuracy: 0.9365\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.18634\n",
            "Epoch 137/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1125 - accuracy: 0.9549 - val_loss: 0.2043 - val_accuracy: 0.9395\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.18634\n",
            "Epoch 138/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1146 - accuracy: 0.9540 - val_loss: 0.2097 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.18634\n",
            "Epoch 139/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1092 - accuracy: 0.9567 - val_loss: 0.2010 - val_accuracy: 0.9365\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.18634\n",
            "Epoch 140/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1075 - accuracy: 0.9557 - val_loss: 0.2177 - val_accuracy: 0.9345\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.18634\n",
            "Epoch 141/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1166 - accuracy: 0.9565 - val_loss: 0.2029 - val_accuracy: 0.9350\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.18634\n",
            "Epoch 142/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1117 - accuracy: 0.9556 - val_loss: 0.2137 - val_accuracy: 0.9345\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.18634\n",
            "Epoch 143/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1085 - accuracy: 0.9542 - val_loss: 0.2080 - val_accuracy: 0.9385\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.18634\n",
            "Epoch 144/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1048 - accuracy: 0.9571 - val_loss: 0.2244 - val_accuracy: 0.9345\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.18634\n",
            "Epoch 145/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1062 - accuracy: 0.9564 - val_loss: 0.2350 - val_accuracy: 0.9335\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.18634\n",
            "Epoch 146/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1114 - accuracy: 0.9561 - val_loss: 0.2167 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.18634\n",
            "Epoch 147/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1086 - accuracy: 0.9564 - val_loss: 0.2218 - val_accuracy: 0.9335\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.18634\n",
            "Epoch 148/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1016 - accuracy: 0.9580 - val_loss: 0.2366 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.18634\n",
            "Epoch 149/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1092 - accuracy: 0.9569 - val_loss: 0.2306 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.18634\n",
            "Epoch 150/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1016 - accuracy: 0.9582 - val_loss: 0.2323 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.18634\n",
            "Epoch 151/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1119 - accuracy: 0.9539 - val_loss: 0.2229 - val_accuracy: 0.9335\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.18634\n",
            "Epoch 152/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1066 - accuracy: 0.9571 - val_loss: 0.2217 - val_accuracy: 0.9380\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.18634\n",
            "Epoch 153/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0938 - accuracy: 0.9634 - val_loss: 0.2338 - val_accuracy: 0.9370\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.18634\n",
            "Epoch 154/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1026 - accuracy: 0.9579 - val_loss: 0.2291 - val_accuracy: 0.9355\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.18634\n",
            "Epoch 155/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1034 - accuracy: 0.9596 - val_loss: 0.2189 - val_accuracy: 0.9355\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.18634\n",
            "Epoch 156/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1094 - accuracy: 0.9576 - val_loss: 0.2138 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.18634\n",
            "Epoch 157/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1008 - accuracy: 0.9584 - val_loss: 0.2268 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.18634\n",
            "Epoch 158/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1054 - accuracy: 0.9570 - val_loss: 0.2371 - val_accuracy: 0.9290\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.18634\n",
            "Epoch 159/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1007 - accuracy: 0.9592 - val_loss: 0.2343 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.18634\n",
            "Epoch 160/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1027 - accuracy: 0.9586 - val_loss: 0.2243 - val_accuracy: 0.9355\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.18634\n",
            "Epoch 161/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1012 - accuracy: 0.9590 - val_loss: 0.2285 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.18634\n",
            "Epoch 162/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0999 - accuracy: 0.9614 - val_loss: 0.2197 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.18634\n",
            "Epoch 163/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1010 - accuracy: 0.9586 - val_loss: 0.2167 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.18634\n",
            "Epoch 164/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0994 - accuracy: 0.9589 - val_loss: 0.2235 - val_accuracy: 0.9390\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.18634\n",
            "Epoch 165/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1005 - accuracy: 0.9574 - val_loss: 0.2203 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.18634\n",
            "Epoch 166/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0966 - accuracy: 0.9601 - val_loss: 0.2287 - val_accuracy: 0.9315\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.18634\n",
            "Epoch 167/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1006 - accuracy: 0.9595 - val_loss: 0.2181 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.18634\n",
            "Epoch 168/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0984 - accuracy: 0.9586 - val_loss: 0.2188 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.18634\n",
            "Epoch 169/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0962 - accuracy: 0.9592 - val_loss: 0.2236 - val_accuracy: 0.9355\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.18634\n",
            "Epoch 170/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0986 - accuracy: 0.9604 - val_loss: 0.2218 - val_accuracy: 0.9320\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.18634\n",
            "Epoch 171/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0975 - accuracy: 0.9605 - val_loss: 0.2266 - val_accuracy: 0.9345\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.18634\n",
            "Epoch 172/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0983 - accuracy: 0.9586 - val_loss: 0.2284 - val_accuracy: 0.9365\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.18634\n",
            "Epoch 173/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0977 - accuracy: 0.9619 - val_loss: 0.2293 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.18634\n",
            "Epoch 174/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0965 - accuracy: 0.9596 - val_loss: 0.2297 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.18634\n",
            "Epoch 175/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0976 - accuracy: 0.9600 - val_loss: 0.2475 - val_accuracy: 0.9300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.18634\n",
            "Epoch 176/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0947 - accuracy: 0.9635 - val_loss: 0.2333 - val_accuracy: 0.9345\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.18634\n",
            "Epoch 177/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0909 - accuracy: 0.9634 - val_loss: 0.2296 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.18634\n",
            "Epoch 178/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0932 - accuracy: 0.9592 - val_loss: 0.2341 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.18634\n",
            "Epoch 179/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0938 - accuracy: 0.9620 - val_loss: 0.2328 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.18634\n",
            "Epoch 180/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0919 - accuracy: 0.9639 - val_loss: 0.2233 - val_accuracy: 0.9350\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.18634\n",
            "Epoch 181/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0968 - accuracy: 0.9613 - val_loss: 0.2357 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.18634\n",
            "Epoch 182/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0884 - accuracy: 0.9647 - val_loss: 0.2234 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.18634\n",
            "Epoch 183/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0926 - accuracy: 0.9629 - val_loss: 0.2243 - val_accuracy: 0.9335\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.18634\n",
            "Epoch 184/200\n",
            "125/125 [==============================] - 1s 4ms/step - loss: 0.0924 - accuracy: 0.9640 - val_loss: 0.2359 - val_accuracy: 0.9320\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.18634\n",
            "Epoch 185/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0900 - accuracy: 0.9635 - val_loss: 0.2431 - val_accuracy: 0.9305\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.18634\n",
            "Epoch 186/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0981 - accuracy: 0.9604 - val_loss: 0.2381 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.18634\n",
            "Epoch 187/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0871 - accuracy: 0.9641 - val_loss: 0.2447 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.18634\n",
            "Epoch 188/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0912 - accuracy: 0.9656 - val_loss: 0.2369 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.18634\n",
            "Epoch 189/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0912 - accuracy: 0.9615 - val_loss: 0.2443 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.18634\n",
            "Epoch 190/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0918 - accuracy: 0.9617 - val_loss: 0.2391 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.18634\n",
            "Epoch 191/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0898 - accuracy: 0.9617 - val_loss: 0.2432 - val_accuracy: 0.9320\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.18634\n",
            "Epoch 192/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.1007 - accuracy: 0.9581 - val_loss: 0.2367 - val_accuracy: 0.9335\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.18634\n",
            "Epoch 193/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0898 - accuracy: 0.9621 - val_loss: 0.2282 - val_accuracy: 0.9350\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.18634\n",
            "Epoch 194/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0835 - accuracy: 0.9664 - val_loss: 0.2610 - val_accuracy: 0.9300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.18634\n",
            "Epoch 195/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0865 - accuracy: 0.9646 - val_loss: 0.2430 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.18634\n",
            "Epoch 196/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0830 - accuracy: 0.9674 - val_loss: 0.2449 - val_accuracy: 0.9305\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.18634\n",
            "Epoch 197/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0854 - accuracy: 0.9656 - val_loss: 0.2417 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.18634\n",
            "Epoch 198/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0930 - accuracy: 0.9620 - val_loss: 0.2229 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.18634\n",
            "Epoch 199/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0817 - accuracy: 0.9671 - val_loss: 0.2444 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.18634\n",
            "Epoch 200/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.0839 - accuracy: 0.9666 - val_loss: 0.2442 - val_accuracy: 0.9360\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.18634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdqByxXRmyYf"
      },
      "source": [
        "model.save(filepath+'Dnn_1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7knHUQm4YMMk",
        "outputId": "b0fa499d-a2f3-47b2-e552-238e619f966e"
      },
      "source": [
        "df = pd.DataFrame(hist.history)\n",
        "df.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loss            0.149220\n",
              "accuracy        0.940156\n",
              "val_loss        0.216911\n",
              "val_accuracy    0.927798\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "8E7Qdi0S2I18",
        "outputId": "f950f58a-f539-45f2-cb0d-30d43514fc93"
      },
      "source": [
        "# visualization\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxJMBiqrn_f3",
        "outputId": "44c6e8e0-1a82-457b-f403-d6209e4ca6c7"
      },
      "source": [
        "from sklearn.metrics import accuracy_score  #\n",
        "score = model.evaluate(X_valid, y_valid, verbose=0)\n",
        "print(int(score[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijopE4p7m4m2"
      },
      "source": [
        "# DNN TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUgR0FGPwTPA",
        "outputId": "98f995f0-c6a8-4ab3-f708-1bec9bac9ff7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "filepath='/content/gdrive/My Drive/'\n",
        "test = pd.read_csv(filepath+'instagram_test.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9npTE_AmzAQ",
        "outputId": "3879aba2-4e5a-4ab6-aff1-742f5219eca6"
      },
      "source": [
        "model = load_model(filepath+'Dnn_1.h5')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 256)               4352      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 34        \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 50,066\n",
            "Trainable params: 49,106\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrndwpiLQnHR",
        "outputId": "f828dac8-75ee-4fb6-b469-d9264bf6cb5c"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(test)\n",
        "train = pd.DataFrame(data = scaler.transform(test), columns = test.columns, index = test.index)\n",
        "y_test = test['#fake']\n",
        "X_test = test.drop('#fake',axis=1)\n",
        "\n",
        "print('X_test = ', X_test.shape)\n",
        "print('y_test = ', y_test.shape)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_test =  (100, 16)\n",
            "y_test =  (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0x0AvfBRR_r",
        "outputId": "965a0729-e260-42f0-a93b-8b87c3a14550"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(int(score[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQF2EIFFIVEg",
        "outputId": "2d024701-a43f-42dc-a619-7609bc56e2c9"
      },
      "source": [
        "#      ?\n",
        "for i in range(1,11):\n",
        "  if y_test[i][0]>y_test[i][1]:\n",
        "    print(\"Real Account\")\n",
        "  else:\n",
        "    print(\"Fake Account!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fake Account!!\n",
            "Fake Account!!\n",
            "Fake Account!!\n",
            "Fake Account!!\n",
            "Real Account\n",
            "Real Account\n",
            "Real Account\n",
            "Real Account\n",
            "Real Account\n",
            "Real Account\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUpDXacl2N0P",
        "outputId": "8ae72595-ae0b-4b68-a868-1b2b73938f90"
      },
      "source": [
        "# TEST     \n",
        "dnn_y = model.predict(X_test[1:11])\n",
        "for i in range(10):\n",
        "  if dnn_y[i][0]>dnn_y[i][1]:\n",
        "    print(\"Real Account\")\n",
        "  else:\n",
        "    print(\"Fake Account!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Real Account\n",
            "Fake Account!!\n",
            "Fake Account!!\n",
            "Fake Account!!\n",
            "Real Account\n",
            "Real Account\n",
            "Real Account\n",
            "Real Account\n",
            "Real Account\n",
            "Real Account\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oyWNXODM87f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}