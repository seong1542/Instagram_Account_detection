# -*- coding: utf-8 -*-
"""인스타그램_딥러닝_실험_softmax기준.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FRMVRqAUSBJUMpxxhYhM3AJSA34eVDot
"""

import numpy as np
import pandas as pd
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
drive.mount('/content/gdrive',force_remount=True)

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
print(train.shape)

y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
X_train.head(10)

from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.3, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)

y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

"""#0. MinMaxScaler써야함 - 비교 통해 보기

***MinMaxScaler 안썼을 때***
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #4
  model.add(Dense(units = (32)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #5
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=100, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)   # 과적합된 것을 볼 수 있음

"""***StandardScaler 썼을 때***"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
print(train.shape)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
train.head()

y_train = train['#fake']
X_train = train.drop('#fake',axis=1)

from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.3, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)

y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #4
  model.add(Dense(units = (32)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #5
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=100, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)    #정확도가 100%? =>문제가 있는 것같음.

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""***MinMaxScaler 썼을 때***"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
print(train.shape)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
train.head()

y_train = train['#fake']
X_train = train.drop('#fake',axis=1)

from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.3, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)

y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #4
  model.add(Dense(units = (32)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #5
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=100, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)  #적당한 것같음.

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""#1. Softmax / Adam / BatchNormalization 쓸 때 적절한 hidden층 찾기

*히든 층 4개 줄 때*
"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #4
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""*히든 층 5개 줄 때*"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #4
  model.add(Dense(units = (32)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #5
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""*히든 층 6개 줄 때*"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #4
  model.add(Dense(units = (32)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #5
  model.add(Dense(units = (16)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #6
  model.add(Dense(units = (8)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""**결론 : 층이 4개일 때가 가장 나은 것같다.**

# Adam 학습률 0.001, 0.01, 0.05 중 어떤 것이 나을까?

***Adam's lr == 0.01***
"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #4
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.01)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""***Adam's lr == 0.005***"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #4
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.005)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""***Adam's lr == 0.001***"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  #hidden layer #4
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""결론 : Adam lr은 0.001을 쓸 때 가장 좋았다.

# Dropout을 쓰면? (0.1, 0.2, 0.3)

**Dropout 0.1**
"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.1))  
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.1))  
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.1))  
  #hidden layer #4
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""**Dropout 0.2**"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.2))  
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.2))  
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.2))  
  #hidden layer #4
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""**Dropout 0.3**"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.3))  
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.3))  
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.3))  
  #hidden layer #4
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""**Dropout 0.4**"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.4))  
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.4))  
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.4))  
  #hidden layer #4
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=64, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""**결론 : Dropout 0.3이 가장 좋음**

# BatchSize 32, 64, 128

**BatchSize 32**
"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.4))  
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.4))  
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.4))  
  #hidden layer #4
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=32, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""**BatchSize 64**


"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.4))  
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.4))  
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.4))  
  #hidden layer #4
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=32, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""**BatchSize 128**"""

filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(train)
train = pd.DataFrame(data = scaler.transform(train), columns = train.columns, index = train.index)
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=2)
print(y_train.shape, y_valid.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout

def dnn_model():
  model=Sequential()

  #hidden layer #1
  model.add(Dense(units = (256), input_dim = 16))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.4))  
  #hidden layer #2
  model.add(Dense(units = (128)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.4))  
  #hidden layer #3
  model.add(Dense(units = (64)))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(Dropout(0.4))  
  #hidden layer #4
  model.add(Dense(units = (16)))
  model.add(Activation('relu'))

  #output layer
  model.add(Dense(units=2))
  model.add(Activation('softmax'))

  return model

model =dnn_model()
model.summary()
opti = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer = opti,loss = 'binary_crossentropy',metrics=['accuracy'])

hist = model.fit(X_train, y_train, epochs=200, verbose=True, batch_size=128, validation_data=(X_valid,y_valid), shuffle=False)

# visualization
fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='test acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='upper right')

plt.show()

"""결론 : Batchsize 64 일때 가장 낫다."""

