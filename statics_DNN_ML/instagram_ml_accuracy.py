# -*- coding: utf-8 -*-
"""instagram_ML_accuracy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16iR0aFRrX0FSCc5OMtXJ3hzMjfJ75LX_

# 가장 정확도 높은 ML모델 찾기
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
filepath='/content/gdrive/My Drive/'
train = pd.read_csv(filepath+'instagram_detection.csv')
print(train.shape)

train.head()

train.info()

# 진짜 계정과 가짜 계정이 총 몇 개씩 있는지 확인 (False = 진짜 계정 , True = 가짜 계정)
unique, freq = np.unique(train['#fake'], return_counts=True)
for i, j in zip(unique, freq):
  print('Label : ',i, ', Frequency : ', j)

train.nunique() # 혹시 0과 1 만 들어있어야 할 곳에 다른 것이 들어 갔는지 확인

"""# int형으로 놓고 실행하면?"""

#모든 속성이 int형일 때 상관관계
data_corr = train.corr(method='pearson')
axes = sns.heatmap(data_corr, vmin=-1, vmax=1, cmap='BrBG')
axes.set_title('Correlation Heatmap Between Features')

y_train = train['#fake']
X_train = train.drop('#fake',axis=1)
X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)

from sklearn.ensemble import RandomForestClassifier
rf= RandomForestClassifier()
rf.fit(X_train, y_train)
train_score = rf.score(X_train, y_train)
valid_score = rf.score(X_valid, y_valid)
print('RandomForest Train_Accuracy : {}'.format(train_score))
print('RandomForest Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.ensemble import GradientBoostingClassifier
gb= GradientBoostingClassifier()
gb.fit(X_train, y_train)
train_score = gb.score(X_train, y_train)
valid_score = gb.score(X_valid, y_valid)
print('GradientBoosting Train_Accuracy : {}'.format(train_score))
print('GradientBoosting Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.linear_model import LogisticRegression
lg = LogisticRegression(penalty = 'l2',solver='newton-cg',random_state=0, max_iter=600)
lg.fit(X_train,y_train)
train_score = lg.score(X_train,y_train)
valid_score = lg.score(X_valid,y_valid)
print('Logistic Train_Accuracy : {}'.format(train_score))
print('Logistic Valid_Accuracy : {}'.format(valid_score))
print()

import xgboost as xgb
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train,y_train)
train_score = xgb_model.score(X_train,y_train)
valid_score = xgb_model.score(X_valid,y_valid)
print('XGBoost Train_Accuracy : {}'.format(train_score))
print('XGBoost Valid_Accuracy : {}'.format(valid_score))
print()

import lightgbm as lgb
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train, y_train)
train_score =lgb_model.score(X_train,y_train)
valid_score= lgb_model.score(X_valid,y_valid)
print('LGBM Train_Accuracy : {}'.format(train_score))
print('LGBM Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train,y_train)
train_score = dt.score(X_train,y_train)
valid_score=dt.score(X_valid,y_valid)
print('DecisionTree Train_Accuracy : {}'.format(train_score))
print('DecisionTree Valid_Accuracy : {}'.format(valid_score))

"""# 속성이 0과 1로만 이루어진 것들 Category/Bool 처리"""

train = pd.read_csv(filepath+'instagram_detection.csv')
print(train.shape)

# 프로필 유무, 이름과 아이디가 같은가, url 유무, 비공개 유무, 프로페셔널 계정 유무, 가짜계정 유무는 범주형으로 설정
train['#profile_pic'] = train['#profile_pic'].astype('category')
train['#name==id'] = train['#name==id'].astype('category')
train['#url'] = train['#url'].astype('category')
train['#private'] = train['#private'].astype('category')
train['#professional'] = train['#professional'].astype('category')
train['#fake'] = train['#fake'].astype('category')
train['#highlight'] = train['#highlight'].astype('category')

train.info()

# train 셋에 있는 fake는 y에 저장
y_train = train['#fake']
X_train = train.drop('#fake',axis=1)

#카테고리처리 했을 때 상관관계
data_corr = X_train.corr(method='pearson')
axes = sns.heatmap(data_corr, vmin=-1, vmax=1, cmap='BrBG')
axes.set_title('Correlation Heatmap Between Features')

# train과 validation set은 8:2로 나눔

X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)

from sklearn.ensemble import RandomForestClassifier
rf= RandomForestClassifier()
rf.fit(X_train, y_train)
train_score = rf.score(X_train, y_train)
valid_score = rf.score(X_valid, y_valid)
print('RandomForest Train_Accuracy : {}'.format(train_score))
print('RandomForest Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.ensemble import GradientBoostingClassifier
gb= GradientBoostingClassifier()
gb.fit(X_train, y_train)
train_score = gb.score(X_train, y_train)
valid_score = gb.score(X_valid, y_valid)
print('GradientBoosting Train_Accuracy : {}'.format(train_score))
print('GradientBoosting Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.linear_model import LogisticRegression
lg = LogisticRegression(penalty = 'l2',solver='newton-cg',random_state=0, max_iter=600)
lg.fit(X_train,y_train)
train_score = lg.score(X_train,y_train)
valid_score = lg.score(X_valid,y_valid)
print('Logistic Train_Accuracy : {}'.format(train_score))
print('Logistic Valid_Accuracy : {}'.format(valid_score))
print()
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train,y_train)
train_score = dt.score(X_train,y_train)
valid_score=dt.score(X_valid,y_valid)
print('DecisionTree Train_Accuracy : {}'.format(train_score))
print('DecisionTree Valid_Accuracy : {}'.format(valid_score))

"""XGBoost와 LGBM은 category 안된다, bool처리해서 실행해봄"""

#XGBoost 시도해보니, category는 안받아줌 -> bool으로 시도해봄.
train['#profile_pic'] = train['#profile_pic'].astype(bool)
train['#name==id'] = train['#name==id'].astype(bool)
train['#url'] = train['#url'].astype(bool)
train['#private'] = train['#private'].astype(bool)
train['#professional'] = train['#professional'].astype(bool)
train['#fake'] = train['#fake'].astype(bool)
train['#highlight'] = train['#highlight'].astype(bool)

train.info()

y_train = train['#fake']
X_train = train.drop('#fake',axis=1)

#Bool처리 했을 때 상관관계
data_corr = X_train.corr(method='pearson')
axes = sns.heatmap(data_corr, vmin=-1, vmax=1, cmap='BrBG')
axes.set_title('Correlation Heatmap Between Features')

X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)

import xgboost as xgb
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train,y_train)
train_score = xgb_model.score(X_train,y_train)
valid_score = xgb_model.score(X_valid,y_valid)
print('XGBoost Train_Accuracy : {}'.format(train_score))
print('XGBoost Valid_Accuracy : {}'.format(valid_score))
print()

import lightgbm as lgb
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train, y_train)
train_score =lgb_model.score(X_train,y_train)
valid_score= lgb_model.score(X_valid,y_valid)
print('LGBM Train_Accuracy : {}'.format(train_score))
print('LGBM Valid_Accuracy : {}'.format(valid_score))
print()

"""# fake부분만 Bool처리"""

train = pd.read_csv(filepath+'instagram_detection.csv')
print(train.shape)

train['#fake'] = train['#fake'].astype('bool')

train.info()

y_train = train['#fake']
X_train = train.drop('#fake',axis=1)

#fake 부분만 Bool처리 했을 때 상관관계
data_corr = X_train.corr(method='pearson')
axes = sns.heatmap(data_corr, vmin=-1, vmax=1, cmap='BrBG')
axes.set_title('Correlation Heatmap Between Features')

X_train,X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2, random_state=0, shuffle=True)

print('X_train = ', X_train.shape)
print('X_valid = ', X_valid.shape)
print('y_train = ', y_train.shape)
print('y_valid = ', y_valid.shape)

from sklearn.ensemble import RandomForestClassifier
rf= RandomForestClassifier()
rf.fit(X_train, y_train)
train_score = rf.score(X_train, y_train)
valid_score = rf.score(X_valid, y_valid)
print('RandomForest Train_Accuracy : {}'.format(train_score))
print('RandomForest Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.ensemble import GradientBoostingClassifier
gb= GradientBoostingClassifier()
gb.fit(X_train, y_train)
train_score = gb.score(X_train, y_train)
valid_score = gb.score(X_valid, y_valid)
print('GradientBoosting Train_Accuracy : {}'.format(train_score))
print('GradientBoosting Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.linear_model import LogisticRegression
lg = LogisticRegression(penalty = 'l2',solver='newton-cg',random_state=0, max_iter=600)
lg.fit(X_train,y_train)
train_score = lg.score(X_train,y_train)
valid_score = lg.score(X_valid,y_valid)
print('Logistic Train_Accuracy : {}'.format(train_score))
print('Logistic Valid_Accuracy : {}'.format(valid_score))
print()

import xgboost as xgb
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train,y_train)
train_score = xgb_model.score(X_train,y_train)
valid_score = xgb_model.score(X_valid,y_valid)
print('XGBoost Train_Accuracy : {}'.format(train_score))
print('XGBoost Valid_Accuracy : {}'.format(valid_score))
print()

import lightgbm as lgb
lgb_model = lgb.LGBMClassifier()
lgb_model.fit(X_train, y_train)
train_score =lgb_model.score(X_train,y_train)
valid_score= lgb_model.score(X_valid,y_valid)
print('LGBM Train_Accuracy : {}'.format(train_score))
print('LGBM Valid_Accuracy : {}'.format(valid_score))
print()

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train,y_train)
train_score = dt.score(X_train,y_train)
valid_score=dt.score(X_valid,y_valid)
print('DecisionTree Train_Accuracy : {}'.format(train_score))
print('DecisionTree Valid_Accuracy : {}'.format(valid_score))

"""결론 : **int형으로만 이루어져있을 때 가장 정확도가 좋음**

# 정확도 높일 수 있을까? -Optuna사용해봄
"""

!pip install optuna #옵튜나 설치

import optuna
from sklearn.metrics import log_loss
categorical_features = ['#profile_pic','#name==id','#url','#private','#highlight','#professional']
def objective(trial):
  params = {'objective':'binary', 
            'n_estimators' : trial.suggest_categorical('n_estimators', [300,500,800,1000]),
            'learning_rate' : trial.suggest_categorical('lerning_rate', [0.05,0.01,0.1]), 
            'max_depth' : trial.suggest_categorical('max_depth', [5,6,7,8]), 
            'max_features' : trial.suggest_categorical('max_features', [0.1,0.3,0.5,0.7]), 
            'min_samples_split' : trial.suggest_categorical('min_samples_split', [2,6]),
            'min_samples_leaf' : trial.suggest_categorical('min_samples_leaf', [2,6])
            }
  lgb_train = lgb.Dataset(X_train,y_train, categorical_feature = categorical_features)
  lgb_eval = lgb.Dataset(X_valid,y_valid,reference=lgb_train, categorical_feature = categorical_features)
  model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_eval],verbose_eval=10, num_boost_round=1000, early_stopping_rounds=100)
  y_pred_valid = model.predict(X_valid,num_iteration=model.best_iteration)
  score = log_loss(y_valid,y_pred_valid)
  return score

study=optuna.create_study(direction='minimize')
study.optimize(objective,n_trials=40) #5회반복

print('Best score : '+str(study.best_trial.value))

study.best_params

import lightgbm as lgb
lgb_model = lgb.LGBMClassifier(**study.best_params)
lgb_model.fit(X_train, y_train)
train_score =lgb_model.score(X_train,y_train)
valid_score= lgb_model.score(X_valid,y_valid)
print('LGBM Train_Accuracy : {}'.format(train_score))
print('LGBM Valid_Accuracy : {}'.format(valid_score))

"""**결론 : 정확도 올리기 위한 optuna작업은 무의미**"""

